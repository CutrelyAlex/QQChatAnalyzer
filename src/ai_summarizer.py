"""
AIæ€»ç»“æ¨¡å— - ä½¿ç”¨OpenAIç”Ÿæˆåˆ›æ„é£æ ¼çš„èŠå¤©æ€»ç»“
"""

import os
import json
import logging
import math
from typing import Dict, List, Any, Optional
from datetime import datetime
from collections import defaultdict

from .prompts import get_system_prompt

logger = logging.getLogger(__name__)

# OpenAI å®¢æˆ·ç«¯
_openai_client = None


def get_openai_client():
    """è·å–æˆ–åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
    global _openai_client
    
    if _openai_client is None:
        try:
            from openai import OpenAI
            
            api_key = os.environ.get('OPENAI_API_KEY', '')
            base_url = os.environ.get('OPENAI_API_BASE', '')
            
            if not api_key:
                return None
            
            kwargs = {'api_key': api_key}
            if base_url:
                kwargs['base_url'] = base_url
            
            _openai_client = OpenAI(**kwargs)
        except ImportError:
            logger.warning("OpenAI library not installed. Run: pip install openai")
            return None
        except Exception as e:
            logger.error(f"Failed to create OpenAI client: {e}")
            return None
    
    return _openai_client


class AISummarizer:
    """
    AIæ€»ç»“å™¨ - ä½¿ç”¨OpenAIç”Ÿæˆåˆ›æ„é£æ ¼çš„èŠå¤©æ€»ç»“
    æ”¯æŒå®Œæ•´èŠå¤©è®°å½•çš„æ™ºèƒ½ç¨€ç–åˆ‡åˆ†
    """
    
    # Tokenä¼°ç®—ç³»æ•°
    CHARS_PER_TOKEN_CN = 1.5  # ä¸­æ–‡å­—ç¬¦çº¦1.5å­—ç¬¦/token
    CHARS_PER_TOKEN_EN = 4.0  # è‹±æ–‡å­—ç¬¦çº¦4å­—ç¬¦/token
    MESSAGE_OVERHEAD = 4      # æ¯æ¡æ¶ˆæ¯çš„é¢å¤–tokenå¼€é”€
    
    # ä¸Šä¸‹æ–‡Tokené¢„ç®—åˆ†é…ï¼ˆåŸºäºæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡ï¼‰
    DEFAULT_CONTEXT_BUDGET = 60000  # é»˜è®¤èŠå¤©æ ·æœ¬Tokené¢„ç®—
    PROMPT_RESERVE = 5000           # ä¸ºç³»ç»Ÿæç¤ºè¯å’Œç»Ÿè®¡æ•°æ®ä¿ç•™çš„Token
    
    def __init__(self, model: str = None, max_tokens: int = 2000, 
                 api_key: str = None, base_url: str = None,
                 context_budget: int = None, timeout: int = None):
        """
        åˆå§‹åŒ–AIæ€»ç»“å™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            max_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆè¾“å‡ºï¼‰
            api_key: OpenAI APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡å¦‚æœªæä¾›ï¼‰
            base_url: OpenAI APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡å¦‚æœªæä¾›ï¼‰
            context_budget: èŠå¤©è®°å½•çš„Tokené¢„ç®—ï¼ˆè¾“å…¥ï¼‰ï¼Œé»˜è®¤60000
            timeout: APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œä»è¯·æ±‚å‘é€åˆ°å®Œå…¨æ¥æ”¶å“åº”ï¼Œé»˜è®¤30ç§’
        """
        self.model = model or os.environ.get('OPENAI_MODEL', 'gpt-4o-mini')
        self.max_tokens = max_tokens
        self.context_budget = context_budget or self.DEFAULT_CONTEXT_BUDGET
        self.timeout = timeout or int(os.environ.get('OPENAI_REQUEST_TIMEOUT', 30))
        
        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰é…ç½®ï¼Œåˆ›å»ºæ–°å®¢æˆ·ç«¯ï¼›å¦åˆ™ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯
        if api_key or base_url:
            self.client = self._create_custom_client(api_key, base_url)
        else:
            self.client = get_openai_client()
    
    def _create_custom_client(self, api_key: str = None, base_url: str = None):
        """åˆ›å»ºè‡ªå®šä¹‰OpenAIå®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            # ä½¿ç”¨æä¾›çš„æˆ–ç¯å¢ƒå˜é‡ä¸­çš„å€¼
            final_api_key = api_key or os.environ.get('OPENAI_API_KEY', '')
            final_base_url = base_url or os.environ.get('OPENAI_API_BASE', '')
            
            if not final_api_key:
                return None
            
            # è®¾ç½®è¶…æ—¶å‚æ•°ï¼šä»è¯·æ±‚å‘é€åˆ°å®Œå…¨æ¥æ”¶å“åº”çš„æ€»è€—æ—¶
            # å¤„ç†å¤§é‡èŠå¤©è®°å½•æ—¶å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            kwargs = {
                'api_key': final_api_key,
                'timeout': self.timeout  # å•ä½ï¼šç§’
            }
            if final_base_url:
                kwargs['base_url'] = final_base_url
            
            return OpenAI(**kwargs)
        except ImportError:
            logger.warning("OpenAI library not installed. Run: pip install openai")
            return None
        except Exception as e:
            logger.error(f"Failed to create OpenAI client: {e}")
            return None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def _estimate_message_tokens(self, content: str) -> int:
        """
        ä¼°ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°
        
        ä½¿ç”¨æ··åˆç­–ç•¥ï¼š
        - ä¸­æ–‡å­—ç¬¦æŒ‰ 1.5 å­—ç¬¦/token
        - è‹±æ–‡/æ•°å­—æŒ‰ 4 å­—ç¬¦/token
        - åŠ ä¸Šæ¶ˆæ¯æ ¼å¼å¼€é”€
        """
        if not content:
            return self.MESSAGE_OVERHEAD
        
        cn_chars = 0
        en_chars = 0
        
        for char in str(content):
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡
                cn_chars += 1
            else:
                en_chars += 1
        
        tokens = (cn_chars / self.CHARS_PER_TOKEN_CN) + \
                 (en_chars / self.CHARS_PER_TOKEN_EN) + \
                 self.MESSAGE_OVERHEAD
        
        return int(math.ceil(tokens))
    
    def _sparse_sample_messages(self, messages: List[Dict[str, Any]], 
                                 target_qq: str = None) -> str:
        """
        æ™ºèƒ½ç¨€ç–é‡‡æ ·èŠå¤©è®°å½•
        
        ç­–ç•¥ï¼š
        1. æŒ‰æ—¥æœŸåˆ†ç»„æ¶ˆæ¯
        2. è®¡ç®—æ€»Tokenæ•°
        3. å¦‚æœè¶…è¿‡é¢„ç®—ï¼ŒæŒ‰æ¯”ä¾‹å‡åŒ€é‡‡æ ·æ—¥æœŸ
        4. åœ¨æ¯ä¸ªé‡‡æ ·æ—¥æœŸå†…ï¼Œå‡åŒ€é‡‡æ ·æ¶ˆæ¯
        
        Args:
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ [{time, sender, qq, content}, ...]
            target_qq: å¯é€‰ï¼Œå¦‚æœæŒ‡å®šåˆ™åªé‡‡æ ·è¯¥QQçš„æ¶ˆæ¯ï¼ˆç”¨äºä¸ªäººåˆ†æï¼‰
        
        Returns:
            æ ¼å¼åŒ–åçš„èŠå¤©è®°å½•å­—ç¬¦ä¸²
        """
        if not messages:
            return ""
        
        # å¯ç”¨äºèŠå¤©è®°å½•çš„Tokené¢„ç®—
        available_budget = self.context_budget - self.PROMPT_RESERVE

        # é¢„ç®—ä¸è¶³æ—¶ç›´æ¥è¿”å›ç©ºæ ·æœ¬
        if available_budget <= 0:
            logger.warning(
                f"Context budget too small (context_budget={self.context_budget}, reserve={self.PROMPT_RESERVE}); returning empty chat sample"
            )
            return ""
        
        # å¦‚æœæŒ‡å®šäº†target_qqï¼Œå…ˆè¿‡æ»¤æ¶ˆæ¯
        if target_qq:
            messages = [m for m in messages if m.get('qq') == target_qq]
        
        if not messages:
            return ""
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        messages_by_date = defaultdict(list)
        for msg in messages:
            time_str = msg.get('time', '')
            try:
                date_str = time_str[:10] if len(time_str) >= 10 else 'unknown'
            except:
                date_str = 'unknown'
            messages_by_date[date_str].append(msg)
        
        # ä¼°ç®—æ€»Tokenæ•°
        total_tokens = 0
        for date_messages in messages_by_date.values():
            for msg in date_messages:
                content = msg.get('content', '')
                sender = msg.get('sender', '')
                time_str = msg.get('time', '')
                # ä¼°ç®—æ ¼å¼åŒ–åçš„Tokenæ•°: [time] sender: content
                formatted = f"[{time_str}] {sender}: {content}"
                total_tokens += self._estimate_message_tokens(formatted)
        
        logger.info(f"Total estimated tokens: {total_tokens}, budget: {available_budget}")
        
        # å¦‚æœåœ¨é¢„ç®—å†…ï¼Œè¿”å›å…¨éƒ¨æ¶ˆæ¯
        if total_tokens <= available_budget:
            return self._format_messages(messages)
        
        # éœ€è¦ç¨€ç–é‡‡æ ·
        retention_ratio = available_budget / total_tokens if total_tokens > 0 else 0
        logger.info(f"Need to prune, retention ratio: {retention_ratio:.2%}")
        
        # è·å–æ‰€æœ‰æ—¥æœŸå¹¶æ’åº
        sorted_dates = sorted(messages_by_date.keys())
        total_days = len(sorted_dates)
        
        # è®¡ç®—ä¿ç•™çš„å¤©æ•°
        keep_days = max(1, int(total_days * retention_ratio))
        
        # å‡åŒ€é‡‡æ ·æ—¥æœŸ
        if keep_days >= total_days:
            selected_dates = sorted_dates
        else:
            step = total_days / keep_days
            selected_indices = [int(i * step) for i in range(keep_days)]
            selected_dates = [sorted_dates[i] for i in selected_indices if i < total_days]
        
        # æ”¶é›†é‡‡æ ·çš„æ¶ˆæ¯ï¼Œå¹¶åœ¨æ¯ä¸ªæ—¥æœŸå†…è¿›ä¸€æ­¥é‡‡æ ·
        sampled_messages = []
        per_day_budget = available_budget // len(selected_dates) if selected_dates else available_budget
        
        for date in selected_dates:
            day_messages = messages_by_date[date]
            day_tokens = sum(
                self._estimate_message_tokens(f"[{m.get('time', '')}] {m.get('sender', '')}: {m.get('content', '')}")
                for m in day_messages
            )
            
            if day_tokens <= per_day_budget:
                # è¿™ä¸€å¤©çš„æ¶ˆæ¯åœ¨é¢„ç®—å†…ï¼Œå…¨éƒ¨ä¿ç•™
                sampled_messages.extend(day_messages)
            else:
                # éœ€è¦åœ¨å¤©å†…è¿›ä¸€æ­¥é‡‡æ ·
                day_retention = per_day_budget / day_tokens
                keep_count = max(1, int(len(day_messages) * day_retention))
                step = len(day_messages) / keep_count
                indices = [int(i * step) for i in range(keep_count)]
                for idx in indices:
                    if idx < len(day_messages):
                        sampled_messages.append(day_messages[idx])
        
        logger.info(f"Sampled {len(sampled_messages)} messages from {len(messages)} total")

        # å…œåº•ï¼šå¦‚æœé‡‡æ ·åä»ç„¶è¶…è¿‡é¢„ç®—ï¼Œå†åšä¸€æ¬¡å…¨å±€å‡åŒ€é‡‡æ ·ï¼ˆä¿è¯è‡³å°‘ç•™ 1 æ¡ï¼‰
        sampled_tokens = sum(
            self._estimate_message_tokens(
                f"[{m.get('time', '')}] {m.get('sender', '')}: {m.get('content', '')}"
            )
            for m in sampled_messages
        )

        if sampled_messages and sampled_tokens > available_budget:
            keep_ratio = available_budget / sampled_tokens if sampled_tokens > 0 else 0
            keep_count = max(1, int(len(sampled_messages) * keep_ratio))
            step = len(sampled_messages) / keep_count if keep_count > 0 else len(sampled_messages)
            indices = [int(i * step) for i in range(keep_count)]
            sampled_messages = [sampled_messages[i] for i in indices if i < len(sampled_messages)]
            logger.info(
                f"Post-prune downsample applied: kept {len(sampled_messages)} messages to better fit budget"
            )
        
        return self._format_messages(sampled_messages)
    
    def _format_messages(self, messages: List[Dict[str, Any]]) -> str:
        """
        å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        lines = []
        for msg in messages:
            time_str = msg.get('time', '')
            sender = msg.get('sender', '')
            content = msg.get('content', '')
            if content:  # åªåŒ…å«æœ‰å†…å®¹çš„æ¶ˆæ¯
                lines.append(f"[{time_str}] {sender}: {content}")
        return '\n'.join(lines)
    
    def generate_personal_summary(self, stats: Dict[str, Any], 
                                   chat_sample: str = "",
                                   messages: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        T051: ç”Ÿæˆä¸ªäººæ€»ç»“ - åˆ›æ„é£æ ¼çš„å¹´åº¦æŠ¥å‘Š
        
        Args:
            stats: PersonalStats.to_dict() çš„ç»“æœ
            chat_sample: å¯é€‰çš„èŠå¤©è®°å½•æ ·æœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ¨èï¼Œä¼šè‡ªåŠ¨è¿›è¡Œæ™ºèƒ½ç¨€ç–é‡‡æ ·ï¼‰
        
        Returns:
            {'success': bool, 'summary': str, 'error': str}
        """
        if not self.is_available():
            return {
                'success': False,
                'summary': '',
                'error': 'AIæœåŠ¡æœªé…ç½®ï¼Œè¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡'
            }
        
        # å¦‚æœæä¾›äº†å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨æ™ºèƒ½ç¨€ç–é‡‡æ ·
        if messages:
            target_qq = stats.get('qq', '')
            chat_sample = self._sparse_sample_messages(messages, target_qq)
        
        prompt = self._build_personal_prompt(stats, chat_sample)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt('personal')},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=0.8  # å¢åŠ åˆ›æ„æ€§
            )
            
            summary = response.choices[0].message.content
            
            return {
                'success': True,
                'summary': summary,
                'tokens_used': response.usage.total_tokens if response.usage else 0,
                'model': self.model
            }
        except Exception as e:
            logger.error(f"Personal summary generation failed: {e}")
            return {
                'success': False,
                'summary': '',
                'error': str(e)
            }
    
    def generate_group_summary(self, group_stats: Dict[str, Any],
                                chat_sample: str = "",
                                messages: List[Dict[str, Any]] = None,
                                network_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        T057: ç”Ÿæˆç¾¤ä½“å’Œç¤¾äº¤ç½‘ç»œèåˆæ€»ç»“
        
        åˆå¹¶ç¾¤ä½“åˆ†æå’Œç½‘ç»œåˆ†æï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆçš„ç¤¾äº¤åˆ†ææŠ¥å‘Š
        
        Args:
            group_stats: GroupStats.to_dict() çš„ç»“æœ
            chat_sample: å¯é€‰çš„èŠå¤©è®°å½•æ ·æœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ¨èï¼Œä¼šè‡ªåŠ¨è¿›è¡Œæ™ºèƒ½ç¨€ç–é‡‡æ ·ï¼‰
            network_stats: NetworkStats.to_dict() çš„ç»“æœï¼ˆå¯é€‰ï¼‰
        
        Returns:
            {'success': bool, 'summary': str, 'error': str}
        """
        if not self.is_available():
            return {
                'success': False,
                'summary': '',
                'error': 'AIæœåŠ¡æœªé…ç½®ï¼Œè¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡'
            }
        
        # å¦‚æœæä¾›äº†å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨æ™ºèƒ½ç¨€ç–é‡‡æ ·
        if messages:
            chat_sample = self._sparse_sample_messages(messages)
        
        prompt = self._build_group_and_network_prompt(group_stats, network_stats, chat_sample)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt('group_and_network')},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=0.8
            )
            
            summary = response.choices[0].message.content
            
            return {
                'success': True,
                'summary': summary,
                'tokens_used': response.usage.total_tokens if response.usage else 0,
                'model': self.model
            }
        except Exception as e:
            logger.error(f"Group and network summary generation failed: {e}")
            return {
                'success': False,
                'summary': '',
                'error': str(e)
            }
    

    def _get_system_prompt(self, summary_type: str) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return get_system_prompt(summary_type)
    
    def _build_personal_prompt(self, stats: Dict[str, Any], 
                                chat_sample: str = "") -> str:
        """æ„å»ºä¸ªäººæ€»ç»“çš„ç”¨æˆ·æç¤ºè¯"""
        
        # æå–å…³é”®æ•°æ®
        nickname = stats.get('nickname', 'ç¥ç§˜ç”¨æˆ·')
        qq = stats.get('qq', 'unknown')
        total_messages = stats.get('total_messages', 0)
        active_days = stats.get('active_days', 0)
        time_dist = stats.get('time_distribution', {})
        user_type = stats.get('user_type', 'æ™®é€šç”¨æˆ·')
        at_count = stats.get('at_count', 0)
        being_at_count = stats.get('being_at_count', 0)
        avg_length = stats.get('avg_message_length', 0)
        image_count = stats.get('image_count', 0)
        emoji_count = stats.get('emoji_count', 0)
        top_words = stats.get('top_words', [])
        max_streak = stats.get('max_streak_days', 0)
        monthly = stats.get('monthly_messages', {})
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶æ®µ
        peak_time = max(time_dist.items(), key=lambda x: x[1])[0] if time_dist else 'æœªçŸ¥'
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æœˆä»½
        peak_month = max(monthly.items(), key=lambda x: x[1])[0] if monthly else 'æœªçŸ¥'
        
        # çƒ­è¯å­—ç¬¦ä¸²
        hot_words_str = ', '.join([w['word'] for w in top_words[:10]]) if top_words else 'æ— '
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·ç”Ÿæˆä¸€ä»½æœ‰è¶£çš„ä¸ªäººèŠå¤©æŠ¥å‘Šï¼š

## ğŸ“Š ç”¨æˆ·æ•°æ®

- **æ˜µç§°**: {nickname}
- **QQå·**: {qq}
- **æ€»æ¶ˆæ¯æ•°**: {total_messages} æ¡
- **æ´»è·ƒå¤©æ•°**: {active_days} å¤©
- **æœ€é•¿è¿ç»­æ´»è·ƒ**: {max_streak} å¤©
- **ç”¨æˆ·ç±»å‹**: {user_type}
- **æœ€æ´»è·ƒæ—¶æ®µ**: {peak_time}
- **æœ€æ´»è·ƒæœˆä»½**: {peak_month}

## ğŸ“ˆ äº’åŠ¨æ•°æ®
- **@åˆ«äººæ¬¡æ•°**: {at_count} æ¬¡
- **è¢«@æ¬¡æ•°**: {being_at_count} æ¬¡
- **å¹³å‡æ¶ˆæ¯é•¿åº¦**: {avg_length:.1f} å­—
- **å‘é€å›¾ç‰‡**: {image_count} å¼ 
- **å‘é€è¡¨æƒ…**: {emoji_count} ä¸ª

## ğŸ”¥ çƒ­è¯TOP10
{hot_words_str}

## â° æ—¶æ®µåˆ†å¸ƒ
{json.dumps(time_dist, ensure_ascii=False, indent=2)}

## ğŸ“… æœˆåº¦æ¶ˆæ¯é‡
{json.dumps(monthly, ensure_ascii=False, indent=2)}
"""
        
        if chat_sample:
            # æ˜¾ç¤ºé‡‡æ ·çš„èŠå¤©è®°å½•ï¼ˆå·²ç»è¿‡ç¨€ç–é‡‡æ ·ï¼Œæ— éœ€å†æˆªæ–­ï¼‰
            prompt += f"""
## ğŸ’¬ èŠå¤©è®°å½•ï¼ˆç”¨äºåˆ†æè¯´è¯é£æ ¼ï¼‰
{chat_sample}
"""
        
        prompt += """
è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½æœ‰è¶£åˆ›æ„çš„ä¸ªäººå¹´åº¦æ€»ç»“ï¼
"""
        
        return prompt
    
    def _build_group_and_network_prompt(self, group_stats: Dict[str, Any],
                                        network_stats: Dict[str, Any] = None,
                                        chat_sample: str = "") -> str:
        """
        æ„å»ºç¾¤ä½“å’Œç½‘ç»œèåˆæ€»ç»“çš„ç”¨æˆ·æç¤ºè¯
        
        Args:
            group_stats: ç¾¤ä½“ç»Ÿè®¡æ•°æ®
            network_stats: ç½‘ç»œç»Ÿè®¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
            chat_sample: èŠå¤©æ ·æœ¬
        
        Returns:
            èåˆçš„ prompt å­—ç¬¦ä¸²
        """
        # ç¾¤ä½“ç»Ÿè®¡æ•°æ®
        total_messages = group_stats.get('total_messages', 0)
        daily_avg = group_stats.get('daily_average', 0)
        peak_hours = group_stats.get('peak_hours', [])
        core_members = group_stats.get('core_members', [])
        active_members = group_stats.get('active_members', [])
        normal_members = group_stats.get('normal_members', [])
        lurkers = group_stats.get('lurkers', [])
        hot_words = group_stats.get('hot_words', [])
        monthly_trend = group_stats.get('monthly_trend', {})
        text_ratio = group_stats.get('text_ratio', 0)
        image_ratio = group_stats.get('image_ratio', 0)
        emoji_ratio = group_stats.get('emoji_ratio', 0)
        
        # æ–°å¢çš„æ—¶é—´ç»Ÿè®¡æ•°æ®ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
        hourly_top_users = group_stats.get('hourly_top_users', {})
        weekday_top_users = group_stats.get('weekday_top_users', {})
        weekday_totals = group_stats.get('weekday_totals', {})
        
        # æ ¸å¿ƒæˆå‘˜ä¿¡æ¯
        core_info = []
        for m in core_members[:5]:
            if isinstance(m, dict):
                core_info.append(f"{m.get('name', m.get('qq', '?'))} ({m.get('count', 0)}æ¡)")
            else:
                core_info.append(str(m))
        
        # çƒ­è¯
        hot_words_str = ', '.join([w['word'] for w in hot_words[:15]]) if hot_words else 'æ— '
        
        # å³°å€¼æ—¶é—´
        peak_str = ', '.join([f"{h}:00" for h in peak_hours[:3]]) if peak_hours else 'æœªçŸ¥'
        
        # æ—¶é—´æ®µæ ‡ç­¾
        time_period_labels = {
            0: 'å‡Œæ™¨ ğŸŒ™',
            1: 'å‡Œæ™¨ ğŸŒ™',
            2: 'å‡Œæ™¨ ğŸŒ™',
            3: 'æ¸…æ™¨ ğŸŒ…',
            4: 'æ¸…æ™¨ ğŸŒ…',
            5: 'æ¸…æ™¨ ğŸŒ…',
            6: 'æ—©ä¸Š â˜€ï¸',
            7: 'æ—©ä¸Š â˜€ï¸',
            8: 'æ—©ä¸Š â˜€ï¸',
            9: 'ä¸Šåˆ ğŸŒ¤ï¸',
            10: 'ä¸Šåˆ ğŸŒ¤ï¸',
            11: 'ä¸Šåˆ ğŸŒ¤ï¸',
            12: 'ä¸­åˆ ğŸŒ',
            13: 'ä¸‹åˆ â˜€ï¸',
            14: 'ä¸‹åˆ â˜€ï¸',
            15: 'ä¸‹åˆ â˜€ï¸',
            16: 'å‚æ™š ğŸŒ†',
            17: 'å‚æ™š ğŸŒ†',
            18: 'å‚æ™š ğŸŒ†',
            19: 'æ™šä¸Š ğŸŒ™',
            20: 'æ™šä¸Š ğŸŒ™',
            21: 'æ™šä¸Š ğŸŒ™',
            22: 'æ·±å¤œ ğŸŒƒ',
            23: 'æ·±å¤œ ğŸŒƒ'
        }
        
        # æ ¼å¼åŒ–æ¯å°æ—¶æœ€æ´»è·ƒç”¨æˆ·
        hourly_info = []
        for hour in sorted(hourly_top_users.keys()):
            user = hourly_top_users[hour]
            if isinstance(user, dict):
                hour_int = int(hour)
                period = time_period_labels.get(hour_int, 'æœªçŸ¥')
                hourly_info.append(f"{period} {hour_int:02d}:00 â†’ {user.get('name', user.get('qq', '?'))} ({user.get('count', 0)}æ¡)")
        hourly_str = '\n'.join(hourly_info) if hourly_info else 'æš‚æ— æ•°æ®'
        
        # æ˜ŸæœŸåç§°
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        
        # æ ¼å¼åŒ–æ¯å‘¨æœ€æ´»è·ƒç”¨æˆ·
        weekday_info = []
        for day in sorted(weekday_top_users.keys()):
            user = weekday_top_users[day]
            if isinstance(user, dict):
                day_name = weekday_names[int(day)] if int(day) < 7 else 'æœªçŸ¥'
                weekday_info.append(f"{day_name}: {user.get('name', user.get('qq', '?'))} ({user.get('count', 0)}æ¡)")
        weekday_str = '\n'.join(weekday_info) if weekday_info else 'æš‚æ— æ•°æ®'
        
        # æ ¼å¼åŒ–æ¯å‘¨æ¶ˆæ¯æ€»é‡
        weekday_totals_info = []
        for day in sorted(weekday_totals.keys()):
            data = weekday_totals[day]
            if isinstance(data, dict):
                count = data.get('count', 0)
                day_name = weekday_names[int(day)] if int(day) < 7 else 'æœªçŸ¥'
                weekday_totals_info.append(f"{day_name}: {count}æ¡")
        weekday_totals_str = '\n'.join(weekday_totals_info) if weekday_totals_info else 'æš‚æ— æ•°æ®'
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç¾¤èŠç”Ÿæˆä¸€ä»½ç»¼åˆçš„ç¤¾äº¤åˆ†æå¹´åº¦æŠ¥å‘Šï¼š

## ğŸ“Š ç¾¤èŠæ´»åŠ›æ•°æ®

- **æ€»æ¶ˆæ¯æ•°**: {total_messages} æ¡
- **æ—¥å‡æ¶ˆæ¯**: {daily_avg:.1f} æ¡
- **æœ€æ´»è·ƒæ—¶æ®µ**: {peak_str}
- **æ¶ˆæ¯ç±»å‹**: æ–‡å­— {text_ratio*100:.1f}% | å›¾ç‰‡ {image_ratio*100:.1f}% | è¡¨æƒ… {emoji_ratio*100:.1f}%

## ğŸ‘¥ æˆå‘˜æ„æˆ
- **æ ¸å¿ƒæˆå‘˜** (TOP 10%): {len(core_members)} äºº
- **æ´»è·ƒæˆå‘˜** (10%-40%): {len(active_members)} äºº
- **æ™®é€šæˆå‘˜** (40%-80%): {len(normal_members)} äºº
- **æ½œæ°´å‘˜** (Bottom 20%): {len(lurkers)} äºº

## ğŸ‘‘ è¯ç—¨æ’è¡Œæ¦œTOP5
{chr(10).join(core_info) if core_info else 'æš‚æ— æ•°æ®'}

## â° æ—¶æ®µæ´»è·ƒåˆ†æï¼ˆæ¯å°æ—¶æœ€æ´»è·ƒäººç‰©ï¼‰
{hourly_str}

## ğŸ“… å‘¨åº¦æ´»è·ƒåˆ†æï¼ˆæ¯å‘¨æœ€æ´»è·ƒäººç‰©ï¼‰
{weekday_str}

## ğŸ“Š å‘¨åº¦æ¶ˆæ¯æ€»é‡åˆ†æ
{weekday_totals_str}

## ğŸ”¥ ç¾¤èŠçƒ­è¯TOP15
{hot_words_str}

## ğŸ“ˆ æœˆåº¦è¶‹åŠ¿
{json.dumps(monthly_trend, ensure_ascii=False, indent=2)}
"""
        
        # å¦‚æœæœ‰ç½‘ç»œç»Ÿè®¡æ•°æ®ï¼Œæ·»åŠ åˆ° prompt
        if network_stats:
            total_nodes = network_stats.get('total_nodes', 0)
            total_edges = network_stats.get('total_edges', 0)
            density = network_stats.get('density', 0)
            avg_clustering = network_stats.get('avg_clustering_coefficient', 0)
            communities = network_stats.get('communities', [])
            most_popular = network_stats.get('most_popular_user', {})
            most_active_pair = network_stats.get('most_active_pair', {})
            key_connectors = network_stats.get('key_connectors', [])
            
            # ç¤¾äº¤ä¸­å¿ƒ
            popular_info = "æ— "
            if most_popular:
                popular_info = f"{most_popular.get('name', most_popular.get('qq', '?'))} (ä¸­å¿ƒåº¦: {most_popular.get('centrality', 0)*100:.1f}%)"
            
            # æœ€ä½³CP
            pair_info = "æ— "
            if most_active_pair:
                pair = most_active_pair.get('pair', [])
                if len(pair) >= 2:
                    pair_info = f"{pair[0]} â†” {pair[1]} (äº’åŠ¨{most_active_pair.get('weight', 0):.0f}æ¬¡)"
            
            # å…³é”®è¿æ¥è€…
            connectors_info = []
            for c in key_connectors[:3]:
                if isinstance(c, dict):
                    connectors_info.append(f"{c.get('name', c.get('qq', '?'))}")
            
            # ç¤¾åŒºä¿¡æ¯
            community_info = f"{len(communities)} ä¸ªå°åœˆå­" if communities else "æš‚æ— æ˜æ˜¾å°åœˆå­"
            
            prompt += f"""
## ğŸ•¸ï¸ ç¤¾äº¤ç½‘ç»œåˆ†æ

- **å‚ä¸äº’åŠ¨çš„æˆå‘˜**: {total_nodes} äºº
- **äº’åŠ¨å…³ç³»æ•°**: {total_edges} æ¡
- **ç½‘ç»œå¯†åº¦**: {density*100:.1f}%
- **å¹³å‡èšç±»ç³»æ•°**: {avg_clustering:.3f}

### ç¤¾äº¤ä¸­å¿ƒï¼ˆäººæ°”ç‹ï¼‰
{popular_info}

### æœ€ä½³CPï¼ˆäº’åŠ¨æœ€å¤šçš„ç»„åˆï¼‰
{pair_info}

### å…³é”®è¿æ¥è€…ï¼ˆç¤¾äº¤æ¡¥æ¢ï¼‰
{', '.join(connectors_info) if connectors_info else 'æš‚æ— æ˜æ˜¾æ¡¥æ¢äººç‰©'}

### å°åœˆå­åˆ†æ
{community_info}
"""
        
        if chat_sample:
            # æ˜¾ç¤ºé‡‡æ ·çš„èŠå¤©è®°å½•ï¼ˆå·²ç»è¿‡ç¨€ç–é‡‡æ ·ï¼Œæ— éœ€å†æˆªæ–­ï¼‰
            prompt += f"""
## ğŸ’¬ èŠå¤©è®°å½•æ ·æœ¬ï¼ˆæ ¸å¿ƒç´ æï¼ç”¨äºåˆ†æç¾¤å‹æ€§æ ¼å’Œè¯´è¯é£æ ¼ï¼‰

âš ï¸ **é‡è¦æç¤º**ï¼šä»¥ä¸‹èŠå¤©è®°å½•æ˜¯åˆ†æçš„æ ¸å¿ƒç´ æï¼è¯·ä»”ç»†é˜…è¯»ï¼Œä»ä¸­æå–æ¯ä¸ªäººçš„ï¼š
- è¯´è¯é£æ ¼
- æ€§æ ¼ç‰¹ç‚¹ï¼ˆå¤–å‘/å†…å‘ã€æ´»æ³¼/ç¨³é‡ã€åæ§½ç³»/æ­£èƒ½é‡ç­‰ç­‰ï¼‰
- æœ‰ä»£è¡¨æ€§çš„é‡‘å¥æˆ–å£å¤´ç¦…ï¼ˆè‡³å°‘2~3å¥è¯ï¼‰
- äº’åŠ¨æ¨¡å¼ï¼ˆå…·ä½“åˆ°å¯¹è¯ï¼‰

{chat_sample}
"""
        
        prompt += """
è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½æœ‰è¶£åˆ›æ„çš„ç»¼åˆå¹´åº¦ç¤¾äº¤åˆ†ææŠ¥å‘Šï¼

**ç‰¹åˆ«è¦æ±‚**ï¼š
1. èŠå¤©è®°å½•æ˜¯æŠ¥å‘Šçš„çµé­‚ï¼ŒåŠ¡å¿…ä»ä¸­æŒ–æ˜æ¯ä¸ªäººçš„ç‹¬ç‰¹æ€§æ ¼
4. æ•°æ®åˆ†æå’ŒèŠå¤©å†…å®¹åˆ†æè¦ç»“åˆèµ·æ¥ï¼Œä¸è¦åªå †æ•°å­—
"""
        
        return prompt
    
