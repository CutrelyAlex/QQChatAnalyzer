"""
AIæ€»ç»“æ¨¡å— - ä½¿ç”¨OpenAIç”Ÿæˆåˆ›æ„é£æ ¼çš„èŠå¤©æ€»ç»“
"""

import os
import json
import logging
import math
from typing import Dict, List, Any, Optional
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger(__name__)

# OpenAI å®¢æˆ·ç«¯
_openai_client = None


def get_openai_client():
    """è·å–æˆ–åˆ›å»ºOpenAIå®¢æˆ·ç«¯"""
    global _openai_client
    
    if _openai_client is None:
        try:
            from openai import OpenAI
            
            api_key = os.environ.get('OPENAI_API_KEY', '')
            base_url = os.environ.get('OPENAI_API_BASE', '')
            
            if not api_key:
                return None
            
            kwargs = {'api_key': api_key}
            if base_url:
                kwargs['base_url'] = base_url
            
            _openai_client = OpenAI(**kwargs)
        except ImportError:
            logger.warning("OpenAI library not installed. Run: pip install openai")
            return None
        except Exception as e:
            logger.error(f"Failed to create OpenAI client: {e}")
            return None
    
    return _openai_client


class AISummarizer:
    """
    AIæ€»ç»“å™¨ - ä½¿ç”¨OpenAIç”Ÿæˆåˆ›æ„é£æ ¼çš„èŠå¤©æ€»ç»“
    æ”¯æŒå®Œæ•´èŠå¤©è®°å½•çš„æ™ºèƒ½ç¨€ç–åˆ‡åˆ†
    """
    
    # Tokenä¼°ç®—ç³»æ•°
    CHARS_PER_TOKEN_CN = 1.5  # ä¸­æ–‡å­—ç¬¦çº¦1.5å­—ç¬¦/token
    CHARS_PER_TOKEN_EN = 4.0  # è‹±æ–‡å­—ç¬¦çº¦4å­—ç¬¦/token
    MESSAGE_OVERHEAD = 4      # æ¯æ¡æ¶ˆæ¯çš„é¢å¤–tokenå¼€é”€
    
    # ä¸Šä¸‹æ–‡Tokené¢„ç®—åˆ†é…ï¼ˆåŸºäºæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡ï¼‰
    DEFAULT_CONTEXT_BUDGET = 60000  # é»˜è®¤èŠå¤©æ ·æœ¬Tokené¢„ç®—
    PROMPT_RESERVE = 5000           # ä¸ºç³»ç»Ÿæç¤ºè¯å’Œç»Ÿè®¡æ•°æ®ä¿ç•™çš„Token
    
    def __init__(self, model: str = None, max_tokens: int = 2000, 
                 api_key: str = None, base_url: str = None,
                 context_budget: int = None, timeout: int = None):
        """
        åˆå§‹åŒ–AIæ€»ç»“å™¨
        
        Args:
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            max_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆè¾“å‡ºï¼‰
            api_key: OpenAI APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡å¦‚æœªæä¾›ï¼‰
            base_url: OpenAI APIåŸºç¡€URLï¼ˆå¯é€‰ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡å¦‚æœªæä¾›ï¼‰
            context_budget: èŠå¤©è®°å½•çš„Tokené¢„ç®—ï¼ˆè¾“å…¥ï¼‰ï¼Œé»˜è®¤60000
            timeout: APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œä»è¯·æ±‚å‘é€åˆ°å®Œå…¨æ¥æ”¶å“åº”ï¼Œé»˜è®¤30ç§’
        """
        self.model = model or os.environ.get('OPENAI_MODEL', 'gpt-4o-mini')
        self.max_tokens = max_tokens
        self.context_budget = context_budget or self.DEFAULT_CONTEXT_BUDGET
        self.timeout = timeout or int(os.environ.get('OPENAI_REQUEST_TIMEOUT', 30))
        
        # å¦‚æœæä¾›äº†è‡ªå®šä¹‰é…ç½®ï¼Œåˆ›å»ºæ–°å®¢æˆ·ç«¯ï¼›å¦åˆ™ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯
        if api_key or base_url:
            self.client = self._create_custom_client(api_key, base_url)
        else:
            self.client = get_openai_client()
    
    def _create_custom_client(self, api_key: str = None, base_url: str = None):
        """åˆ›å»ºè‡ªå®šä¹‰OpenAIå®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            # ä½¿ç”¨æä¾›çš„æˆ–ç¯å¢ƒå˜é‡ä¸­çš„å€¼
            final_api_key = api_key or os.environ.get('OPENAI_API_KEY', '')
            final_base_url = base_url or os.environ.get('OPENAI_API_BASE', '')
            
            if not final_api_key:
                return None
            
            # è®¾ç½®è¶…æ—¶å‚æ•°ï¼šä»è¯·æ±‚å‘é€åˆ°å®Œå…¨æ¥æ”¶å“åº”çš„æ€»è€—æ—¶
            # å¤„ç†å¤§é‡èŠå¤©è®°å½•æ—¶å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
            kwargs = {
                'api_key': final_api_key,
                'timeout': self.timeout  # å•ä½ï¼šç§’
            }
            if final_base_url:
                kwargs['base_url'] = final_base_url
            
            return OpenAI(**kwargs)
        except ImportError:
            logger.warning("OpenAI library not installed. Run: pip install openai")
            return None
        except Exception as e:
            logger.error(f"Failed to create OpenAI client: {e}")
            return None
    
    def is_available(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    def _estimate_message_tokens(self, content: str) -> int:
        """
        ä¼°ç®—å•æ¡æ¶ˆæ¯çš„tokenæ•°
        
        ä½¿ç”¨æ··åˆç­–ç•¥ï¼š
        - ä¸­æ–‡å­—ç¬¦æŒ‰ 1.5 å­—ç¬¦/token
        - è‹±æ–‡/æ•°å­—æŒ‰ 4 å­—ç¬¦/token
        - åŠ ä¸Šæ¶ˆæ¯æ ¼å¼å¼€é”€
        """
        if not content:
            return self.MESSAGE_OVERHEAD
        
        cn_chars = 0
        en_chars = 0
        
        for char in str(content):
            if '\u4e00' <= char <= '\u9fff':  # ä¸­æ–‡
                cn_chars += 1
            else:
                en_chars += 1
        
        tokens = (cn_chars / self.CHARS_PER_TOKEN_CN) + \
                 (en_chars / self.CHARS_PER_TOKEN_EN) + \
                 self.MESSAGE_OVERHEAD
        
        return int(math.ceil(tokens))
    
    def _sparse_sample_messages(self, messages: List[Dict[str, Any]], 
                                 target_qq: str = None) -> str:
        """
        æ™ºèƒ½ç¨€ç–é‡‡æ ·èŠå¤©è®°å½•
        
        ç­–ç•¥ï¼š
        1. æŒ‰æ—¥æœŸåˆ†ç»„æ¶ˆæ¯
        2. è®¡ç®—æ€»Tokenæ•°
        3. å¦‚æœè¶…è¿‡é¢„ç®—ï¼ŒæŒ‰æ¯”ä¾‹å‡åŒ€é‡‡æ ·æ—¥æœŸ
        4. åœ¨æ¯ä¸ªé‡‡æ ·æ—¥æœŸå†…ï¼Œå‡åŒ€é‡‡æ ·æ¶ˆæ¯
        
        Args:
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ [{time, sender, qq, content}, ...]
            target_qq: å¯é€‰ï¼Œå¦‚æœæŒ‡å®šåˆ™åªé‡‡æ ·è¯¥QQçš„æ¶ˆæ¯ï¼ˆç”¨äºä¸ªäººåˆ†æï¼‰
        
        Returns:
            æ ¼å¼åŒ–åçš„èŠå¤©è®°å½•å­—ç¬¦ä¸²
        """
        if not messages:
            return ""
        
        # å¯ç”¨äºèŠå¤©è®°å½•çš„Tokené¢„ç®—
        available_budget = self.context_budget - self.PROMPT_RESERVE
        
        # å¦‚æœæŒ‡å®šäº†target_qqï¼Œå…ˆè¿‡æ»¤æ¶ˆæ¯
        if target_qq:
            messages = [m for m in messages if m.get('qq') == target_qq]
        
        if not messages:
            return ""
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        messages_by_date = defaultdict(list)
        for msg in messages:
            time_str = msg.get('time', '')
            try:
                date_str = time_str[:10] if len(time_str) >= 10 else 'unknown'
            except:
                date_str = 'unknown'
            messages_by_date[date_str].append(msg)
        
        # ä¼°ç®—æ€»Tokenæ•°
        total_tokens = 0
        for date_messages in messages_by_date.values():
            for msg in date_messages:
                content = msg.get('content', '')
                sender = msg.get('sender', '')
                time_str = msg.get('time', '')
                # ä¼°ç®—æ ¼å¼åŒ–åçš„Tokenæ•°: [time] sender: content
                formatted = f"[{time_str}] {sender}: {content}"
                total_tokens += self._estimate_message_tokens(formatted)
        
        logger.info(f"Total estimated tokens: {total_tokens}, budget: {available_budget}")
        
        # å¦‚æœåœ¨é¢„ç®—å†…ï¼Œè¿”å›å…¨éƒ¨æ¶ˆæ¯
        if total_tokens <= available_budget:
            return self._format_messages(messages)
        
        # éœ€è¦ç¨€ç–é‡‡æ ·
        retention_ratio = available_budget / total_tokens
        logger.info(f"Need to prune, retention ratio: {retention_ratio:.2%}")
        
        # è·å–æ‰€æœ‰æ—¥æœŸå¹¶æ’åº
        sorted_dates = sorted(messages_by_date.keys())
        total_days = len(sorted_dates)
        
        # è®¡ç®—ä¿ç•™çš„å¤©æ•°
        keep_days = max(1, int(total_days * retention_ratio))
        
        # å‡åŒ€é‡‡æ ·æ—¥æœŸ
        if keep_days >= total_days:
            selected_dates = sorted_dates
        else:
            step = total_days / keep_days
            selected_indices = [int(i * step) for i in range(keep_days)]
            selected_dates = [sorted_dates[i] for i in selected_indices if i < total_days]
        
        # æ”¶é›†é‡‡æ ·çš„æ¶ˆæ¯ï¼Œå¹¶åœ¨æ¯ä¸ªæ—¥æœŸå†…è¿›ä¸€æ­¥é‡‡æ ·
        sampled_messages = []
        per_day_budget = available_budget // len(selected_dates) if selected_dates else available_budget
        
        for date in selected_dates:
            day_messages = messages_by_date[date]
            day_tokens = sum(
                self._estimate_message_tokens(f"[{m.get('time', '')}] {m.get('sender', '')}: {m.get('content', '')}")
                for m in day_messages
            )
            
            if day_tokens <= per_day_budget:
                # è¿™ä¸€å¤©çš„æ¶ˆæ¯åœ¨é¢„ç®—å†…ï¼Œå…¨éƒ¨ä¿ç•™
                sampled_messages.extend(day_messages)
            else:
                # éœ€è¦åœ¨å¤©å†…è¿›ä¸€æ­¥é‡‡æ ·
                day_retention = per_day_budget / day_tokens
                keep_count = max(1, int(len(day_messages) * day_retention))
                step = len(day_messages) / keep_count
                indices = [int(i * step) for i in range(keep_count)]
                for idx in indices:
                    if idx < len(day_messages):
                        sampled_messages.append(day_messages[idx])
        
        logger.info(f"Sampled {len(sampled_messages)} messages from {len(messages)} total")
        
        return self._format_messages(sampled_messages)
    
    def _format_messages(self, messages: List[Dict[str, Any]]) -> str:
        """
        å°†æ¶ˆæ¯åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
        """
        lines = []
        for msg in messages:
            time_str = msg.get('time', '')
            sender = msg.get('sender', '')
            content = msg.get('content', '')
            if content:  # åªåŒ…å«æœ‰å†…å®¹çš„æ¶ˆæ¯
                lines.append(f"[{time_str}] {sender}: {content}")
        return '\n'.join(lines)
    
    def generate_personal_summary(self, stats: Dict[str, Any], 
                                   chat_sample: str = "",
                                   messages: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        T051: ç”Ÿæˆä¸ªäººæ€»ç»“ - åˆ›æ„é£æ ¼çš„å¹´åº¦æŠ¥å‘Š
        
        Args:
            stats: PersonalStats.to_dict() çš„ç»“æœ
            chat_sample: å¯é€‰çš„èŠå¤©è®°å½•æ ·æœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ¨èï¼Œä¼šè‡ªåŠ¨è¿›è¡Œæ™ºèƒ½ç¨€ç–é‡‡æ ·ï¼‰
        
        Returns:
            {'success': bool, 'summary': str, 'error': str}
        """
        if not self.is_available():
            return {
                'success': False,
                'summary': '',
                'error': 'AIæœåŠ¡æœªé…ç½®ï¼Œè¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡'
            }
        
        # å¦‚æœæä¾›äº†å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨æ™ºèƒ½ç¨€ç–é‡‡æ ·
        if messages:
            target_qq = stats.get('qq', '')
            chat_sample = self._sparse_sample_messages(messages, target_qq)
        
        prompt = self._build_personal_prompt(stats, chat_sample)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt('personal')},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=0.8  # å¢åŠ åˆ›æ„æ€§
            )
            
            summary = response.choices[0].message.content
            
            return {
                'success': True,
                'summary': summary,
                'tokens_used': response.usage.total_tokens if response.usage else 0,
                'model': self.model
            }
        except Exception as e:
            logger.error(f"Personal summary generation failed: {e}")
            return {
                'success': False,
                'summary': '',
                'error': str(e)
            }
    
    def generate_group_summary(self, group_stats: Dict[str, Any],
                                chat_sample: str = "",
                                messages: List[Dict[str, Any]] = None,
                                network_stats: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        T057: ç”Ÿæˆç¾¤ä½“å’Œç¤¾äº¤ç½‘ç»œèåˆæ€»ç»“
        
        åˆå¹¶ç¾¤ä½“åˆ†æå’Œç½‘ç»œåˆ†æï¼Œç”Ÿæˆä¸€ä»½ç»¼åˆçš„ç¤¾äº¤åˆ†ææŠ¥å‘Š
        
        Args:
            group_stats: GroupStats.to_dict() çš„ç»“æœ
            chat_sample: å¯é€‰çš„èŠå¤©è®°å½•æ ·æœ¬ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
            messages: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨ï¼ˆæ¨èï¼Œä¼šè‡ªåŠ¨è¿›è¡Œæ™ºèƒ½ç¨€ç–é‡‡æ ·ï¼‰
            network_stats: NetworkStats.to_dict() çš„ç»“æœï¼ˆå¯é€‰ï¼‰
        
        Returns:
            {'success': bool, 'summary': str, 'error': str}
        """
        if not self.is_available():
            return {
                'success': False,
                'summary': '',
                'error': 'AIæœåŠ¡æœªé…ç½®ï¼Œè¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡'
            }
        
        # å¦‚æœæä¾›äº†å®Œæ•´æ¶ˆæ¯åˆ—è¡¨ï¼Œä½¿ç”¨æ™ºèƒ½ç¨€ç–é‡‡æ ·
        if messages:
            chat_sample = self._sparse_sample_messages(messages)
        
        prompt = self._build_group_and_network_prompt(group_stats, network_stats, chat_sample)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_prompt('group_and_network')},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=0.8
            )
            
            summary = response.choices[0].message.content
            
            return {
                'success': True,
                'summary': summary,
                'tokens_used': response.usage.total_tokens if response.usage else 0,
                'model': self.model
            }
        except Exception as e:
            logger.error(f"Group and network summary generation failed: {e}")
            return {
                'success': False,
                'summary': '',
                'error': str(e)
            }
    

    def _get_system_prompt(self, summary_type: str) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        
        base_style = """
ä½ æ˜¯ä¸€ä¸ªè¶…çº§æœ‰è¶£çš„èŠå¤©è®°å½•åˆ†æå¸ˆï¼Œé£æ ¼åŒ…å«ï¼š
- ğŸ® è¾£è¯„ï¼šæ¯’èˆŒä½†ä¸ä¼¤äººï¼Œåæ§½ä¸­å¸¦ç€çˆ±
- ğŸµ æ¸©æƒ…è„‰è„‰ï¼šå……æ»¡ä»ªå¼æ„Ÿå’Œäººæƒ…å‘³
- ğŸ“± æ•°æ®å¯è§†åŒ–é£æ ¼ï¼šç”¨æ•°æ®è®²æ•…äº‹

å†™ä½œè¦æ±‚ï¼š
1. ä½¿ç”¨æœ‰è¶£çš„è¯­è¨€ï¼Œè®©æŠ¥å‘Šç”ŸåŠ¨æœ‰è¶£
2. åˆ›é€ é’ˆå¯¹å•ä¸ªç”¨æˆ·ï¼ˆæ˜µç§°ç§°å‘¼ï¼Œä½†ç”¨QQå·åŒºåˆ†ï¼‰ä¸ªæ€§åŒ–çš„"ç§°å·"å’Œ"æˆå°±å¾½ç« "
3. ç”¨ç½‘ç»œçƒ­æ¢—å’Œæµè¡Œè¯­ï¼Œä½†ä¸è¦å¤ªè¿‡æ—¶
4. æ•°æ®è¦å…·ä½“ï¼Œä½†è¡¨è¾¾è¦æœ‰è¶£
5. é€‚åº¦æ¯’èˆŒåæ§½ï¼Œä½†è¦è®©äººä¼šå¿ƒä¸€ç¬‘è€Œä¸æ˜¯ç”Ÿæ°”
6. æœ€åç»™ä¸€ä¸ªæ€»ç»“

è¾“å‡ºæ ¼å¼ï¼šä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«æ ‡é¢˜ã€åŠ ç²—ç­‰
"""
        
        type_specific = {
            'personal': """
## ä¸ªäººæŠ¥å‘Šç‰¹æ®Šè¦æ±‚ï¼š

æ ¹æ®ç”¨æˆ·çš„èŠå¤©æ•°æ®ï¼Œç”Ÿæˆä¸€ä»½ä¸ªæ€§åŒ–çš„"å¹´åº¦äººè®¾æŠ¥å‘Š"ï¼ŒåŒ…å«ï¼š

1. **å¼€åœºç™½** - ç”¨ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªäººçš„ç¾¤èŠäººè®¾
2. **ä¸“å±ç§°å·** - æ ¹æ®æ•°æ®ç»™å‡º2-3ä¸ªæœ‰è¶£çš„ç§°å·ï¼Œä¾‹å¦‚ï¼š
   - ğŸŒ™ "å‡Œæ™¨ä¸‰ç‚¹ã®ç¾¤èŠå®ˆæŠ¤è€…" (å¦‚æœæ·±å¤œæ´»è·ƒ)
   - ğŸ” "æ—©èµ·æ‰“å¡ç¬¬ä¸€äºº" (å¦‚æœæ—©èµ·æ´»è·ƒ)  
   - ğŸ’¬ "æ—¥å‡999+æ¶ˆæ¯ã®è¯ç—¨" (å¦‚æœæ¶ˆæ¯å¤š)
   - ğŸ¤« "ç¥ç§˜æ½œæ°´å‘˜" (å¦‚æœæ¶ˆæ¯å°‘)
   - ğŸ“¸ "è¡¨æƒ…åŒ…ã®ä¼ æ•™å£«" (å¦‚æœè¡¨æƒ…å¤š)

3. **æ•°æ®äº®ç‚¹** - æŒ‘é€‰æœ€æœ‰è¶£çš„2-3ä¸ªæ•°æ®ç‚¹ï¼Œç”¨æœ‰è¶£çš„æ–¹å¼å‘ˆç°
4. **ç¾¤èŠç”»é£åˆ†æ** - æ ¹æ®çƒ­è¯å’Œæ¶ˆæ¯ç‰¹ç‚¹åˆ†ætaçš„è¯´è¯é£æ ¼
5. **å¹´åº¦é‡‘å¥** - å†™ä¸€æ®µæ€§æ ¼åˆ†æï¼š
   - ğŸ¯ **ä¸“å±äººè®¾æ ‡ç­¾** - ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªäººçš„ç¾¤èŠå½¢è±¡
   - ğŸ’¬ **è¯´è¯é£æ ¼** - taé€šå¸¸æ€ä¹ˆè¯´è¯ï¼Ÿæ˜¯ç®€æ´è¿˜æ˜¯å•°å—¦ï¼Ÿæ­£ç»è¿˜æ˜¯ææ€ªï¼Ÿ
   - ğŸ­ **æ€§æ ¼ç‰¹ç‚¹** - ä»èŠå¤©å†…å®¹åˆ†ætaçš„æ€§æ ¼ï¼šçƒ­æƒ…/å†·æ·¡ã€è¯å¤š/è¯å°‘ã€çˆ±åæ§½/æ­£èƒ½é‡ç­‰
   - ğŸ“ **ä»£è¡¨æ€§é‡‘å¥** - ä»èŠå¤©æ ·æœ¬ä¸­æ‰¾å‡ºæœ€èƒ½ä»£è¡¨taé£æ ¼çš„ä¸€å¥è¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
   - ğŸ† **ä¸“å±æˆå°±** - ç»™taä¸€ä¸ªé‡èº«å®šåˆ¶çš„æç¬‘æˆå°±/å¤´è¡”
6. **æ¯’èˆŒåæ§½** - ä¸€å°æ®µå‹å–„çš„åæ§½
""",
            'group_and_network': """
## ç¾¤ä½“å’Œç¤¾äº¤ç½‘ç»œèåˆæŠ¥å‘Šç‰¹æ®Šè¦æ±‚ï¼š

ç”Ÿæˆä¸€ä»½ç»¼åˆçš„"å¹´åº¦ç¤¾äº¤åˆ†ææŠ¥å‘Š"ï¼Œèåˆç¾¤ä½“æ´»åŠ›å’Œäººé™…å…³ç³»ï¼Œ**é‡ç‚¹å…³æ³¨èŠå¤©è®°å½•å†…å®¹ï¼Œæ·±å…¥åˆ†ææ¯ä¸ªäººçš„æ€§æ ¼ç‰¹ç‚¹**ã€‚

### ğŸ“Š ç¬¬ä¸€éƒ¨åˆ†ï¼šç¾¤èŠæ¡£æ¡ˆä¸æ´»åŠ›æŒ‡æ•°
1. **ç¾¤èŠæ¡£æ¡ˆ** - ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªç¾¤çš„æ°”è´¨
2. **ç¾¤æ´»åŠ›æŒ‡æ•°** - æ ¹æ®æ¶ˆæ¯é‡è¯„çº§ï¼Œç»™ä¸ªæœ‰è¶£çš„è¯„è¯­å¦‚ï¼š
   - ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ "æ¯”æ˜¥æ™šå¼¹å¹•è¿˜çƒ­é—¹"
   - ğŸ”¥ğŸ”¥ğŸ”¥ "ä¸‰å¤©ä¸çœ‹å°±999+"
   - ğŸ”¥ "å®‰é™å¾—åƒä¸ªå­¦ä¹ ç¾¤"
3. **å¹´åº¦MVPæ¦œå•** - ç»™æ ¸å¿ƒæˆå‘˜é¢å¥–ï¼ˆè¯ç—¨ä¹‹ç‹ã€æ·±å¤œå®ˆæŠ¤è€…ã€æ—©èµ·å·ç‹ã€è¡¨æƒ…åŒ…å¤§æˆ·ç­‰ï¼‰

### ğŸ‘¤ ç¬¬äºŒéƒ¨åˆ†ï¼šç¾¤å‹æ€§æ ¼ç”»åƒï¼ˆæ ¸å¿ƒå†…å®¹ï¼ï¼‰
**è¿™æ˜¯æŠ¥å‘Šçš„é‡ç‚¹ï¼** æ ¹æ®èŠå¤©è®°å½•ï¼Œä¸ºæ¯ä¸ªæ´»è·ƒæˆå‘˜ç”Ÿæˆç‹¬ç‰¹çš„æ€§æ ¼åˆ†æï¼š

4. **ç¾¤å‹æ€§æ ¼å¤§èµ** - ä¸ºæ’åå‰5-8ä½çš„æ´»è·ƒæˆå‘˜åˆ†åˆ«å†™ä¸€æ®µæ€§æ ¼åˆ†æï¼š
   - ğŸ¯ **ä¸“å±äººè®¾æ ‡ç­¾** - ä¸€å¥è¯æ¦‚æ‹¬è¿™ä¸ªäººçš„ç¾¤èŠå½¢è±¡
   - ğŸ’¬ **è¯´è¯é£æ ¼** - taé€šå¸¸æ€ä¹ˆè¯´è¯ï¼Ÿæ˜¯ç®€æ´è¿˜æ˜¯å•°å—¦ï¼Ÿæ­£ç»è¿˜æ˜¯ææ€ªï¼Ÿ
   - ğŸ­ **æ€§æ ¼ç‰¹ç‚¹** - ä»èŠå¤©å†…å®¹åˆ†ætaçš„æ€§æ ¼ï¼šçƒ­æƒ…/å†·æ·¡ã€è¯å¤š/è¯å°‘ã€çˆ±åæ§½/æ­£èƒ½é‡ç­‰
   - ğŸ“ **ä»£è¡¨æ€§é‡‘å¥** - ä»èŠå¤©æ ·æœ¬ä¸­æ‰¾å‡ºæœ€èƒ½ä»£è¡¨taé£æ ¼çš„ä¸€å¥è¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
   - ğŸ† **ä¸“å±æˆå°±** - ç»™taä¸€ä¸ªé‡èº«å®šåˆ¶çš„æç¬‘æˆå°±/å¤´è¡”

5. **ç¾¤å‹CPé€Ÿé…æ¦œ** - æ ¹æ®äº’åŠ¨å…³ç³»ï¼Œç»™äº’åŠ¨æœ€å¤šçš„å‡ å¯¹ç»„åˆèµ·CPåï¼Œåˆ†æä»–ä»¬çš„äº’åŠ¨æ¨¡å¼

### ğŸ‘¥ ç¬¬ä¸‰éƒ¨åˆ†ï¼šäººé™…å…³ç³»ä¸ç¤¾äº¤ç½‘ç»œ
6. **ç¤¾äº¤ä¸­å¿ƒï¼ˆäººæ°”ç‹ï¼‰** - è°æ˜¯ç¾¤é‡Œçš„ç¤¾äº¤è¾¾äººï¼Œç»™taä¸€ä¸ªæœ‰è¶£çš„ç§°å·
7. **æœ€ä½³CP** - äº’åŠ¨æœ€å¤šçš„ç»„åˆï¼Œç»™ä»–ä»¬èµ·ä¸ªCPåï¼Œåæ§½ä»–ä»¬çš„äº’åŠ¨é£æ ¼
8. **å°åœˆå­åˆ†æ** - ç¾¤é‡Œæœ‰å“ªäº›å°å›¢ä½“æˆ–æ´¾ç³»ï¼Œç®€å•æè¿°ä»–ä»¬çš„ç‰¹ç‚¹
9. **ç¤¾äº¤è¾¾äººå»ºè®®** - ç»™æ½œæ°´å…šæˆ–å†·åœºäººçš„å‹å¥½å»ºè®®ï¼ˆè°ƒä¾ƒå‘ï¼Œä¸è¦å¤ªæ‰å¿ƒï¼‰

### ğŸ¯ ç¬¬å››éƒ¨åˆ†ï¼šç»¼åˆåˆ†æä¸æ€»ç»“
10. **ç¾¤èŠçƒ­è¯äº‘** - TOP çƒ­è¯ä½“ç°çš„ç¾¤æ–‡åŒ–
11. **æ´»è·ƒæ—¶é—´æ®µåˆ†æ** - è¿™ä¸ªç¾¤ä»€ä¹ˆæ—¶å€™æœ€æ´»è·ƒï¼Œç»™ä¸ªæœ‰è¶£çš„è§£è¯»
12. **å¹´åº¦å¤§äº‹è®°** - æ ¹æ®æœˆåº¦è¶‹åŠ¿å’ŒèŠå¤©è®°å½•çŒœæµ‹ç¾¤é‡Œå‘ç”Ÿè¿‡ä»€ä¹ˆæœ‰è¶£çš„äº‹
13. **ç¾¤èŠç”»é£é‰´å®šä¸ç¤¾äº¤æ°›å›´æ€»ç»“** - è¿™æ˜¯ä¸ªä»€ä¹ˆç±»å‹çš„ç¾¤ï¼Œç¤¾äº¤æ°›å›´å¦‚ä½•

### âœ¨ æ•´ä½“é£æ ¼è¦æ±‚ï¼š
- **èŠå¤©è®°å½•æ˜¯æ ¸å¿ƒç´ æ**ï¼šä»”ç»†é˜…è¯»èŠå¤©æ ·æœ¬ï¼Œä»ä¸­æŒ–æ˜æ¯ä¸ªäººçš„è¯´è¯é£æ ¼å’Œæ€§æ ¼
- **ä¸ªæ€§åŒ–åˆ†æ**ï¼šä¸è¦æ³›æ³›è€Œè°ˆï¼Œè¦é’ˆå¯¹å…·ä½“çš„äººè¯´å…·ä½“çš„è¯
- èåˆç¾¤ä½“çƒ­åº¦å’Œäººé™…æ¸©åº¦ï¼Œæ—¢è¦ä½“ç°æ´»åŠ›æŒ‡æ•°ï¼Œä¹Ÿè¦æŒ–æ˜äººæƒ…å‘³
- é¿å…å†·å†°å†°çš„æ•°æ®åˆ†æï¼Œç”¨æ•…äº‹å’Œè¶£äº‹æ¥è¯ é‡Šæ•°æ®
- å¯¹æ¯ä¸ªäººå’Œæ´¾ç³»çš„æè¿°è¦æœ‰ä¸ªæ€§ï¼Œè®©äººçœ‹äº†ä¼šå¿ƒä¸€ç¬‘
"""
      }
        
        return base_style + type_specific.get(summary_type, '')
    
    def _build_personal_prompt(self, stats: Dict[str, Any], 
                                chat_sample: str = "") -> str:
        """æ„å»ºä¸ªäººæ€»ç»“çš„ç”¨æˆ·æç¤ºè¯"""
        
        # æå–å…³é”®æ•°æ®
        nickname = stats.get('nickname', 'ç¥ç§˜ç”¨æˆ·')
        qq = stats.get('qq', 'unknown')
        total_messages = stats.get('total_messages', 0)
        active_days = stats.get('active_days', 0)
        time_dist = stats.get('time_distribution', {})
        user_type = stats.get('user_type', 'æ™®é€šç”¨æˆ·')
        at_count = stats.get('at_count', 0)
        being_at_count = stats.get('being_at_count', 0)
        avg_length = stats.get('avg_message_length', 0)
        image_count = stats.get('image_count', 0)
        emoji_count = stats.get('emoji_count', 0)
        top_words = stats.get('top_words', [])
        max_streak = stats.get('max_streak_days', 0)
        monthly = stats.get('monthly_messages', {})
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æ—¶æ®µ
        peak_time = max(time_dist.items(), key=lambda x: x[1])[0] if time_dist else 'æœªçŸ¥'
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„æœˆä»½
        peak_month = max(monthly.items(), key=lambda x: x[1])[0] if monthly else 'æœªçŸ¥'
        
        # çƒ­è¯å­—ç¬¦ä¸²
        hot_words_str = ', '.join([w['word'] for w in top_words[:10]]) if top_words else 'æ— '
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç”¨æˆ·ç”Ÿæˆä¸€ä»½æœ‰è¶£çš„ä¸ªäººèŠå¤©æŠ¥å‘Šï¼š

## ğŸ“Š ç”¨æˆ·æ•°æ®

- **æ˜µç§°**: {nickname}
- **QQå·**: {qq}
- **æ€»æ¶ˆæ¯æ•°**: {total_messages} æ¡
- **æ´»è·ƒå¤©æ•°**: {active_days} å¤©
- **æœ€é•¿è¿ç»­æ´»è·ƒ**: {max_streak} å¤©
- **ç”¨æˆ·ç±»å‹**: {user_type}
- **æœ€æ´»è·ƒæ—¶æ®µ**: {peak_time}
- **æœ€æ´»è·ƒæœˆä»½**: {peak_month}

## ğŸ“ˆ äº’åŠ¨æ•°æ®
- **@åˆ«äººæ¬¡æ•°**: {at_count} æ¬¡
- **è¢«@æ¬¡æ•°**: {being_at_count} æ¬¡
- **å¹³å‡æ¶ˆæ¯é•¿åº¦**: {avg_length:.1f} å­—
- **å‘é€å›¾ç‰‡**: {image_count} å¼ 
- **å‘é€è¡¨æƒ…**: {emoji_count} ä¸ª

## ğŸ”¥ çƒ­è¯TOP10
{hot_words_str}

## â° æ—¶æ®µåˆ†å¸ƒ
{json.dumps(time_dist, ensure_ascii=False, indent=2)}

## ğŸ“… æœˆåº¦æ¶ˆæ¯é‡
{json.dumps(monthly, ensure_ascii=False, indent=2)}
"""
        
        if chat_sample:
            # æ˜¾ç¤ºé‡‡æ ·çš„èŠå¤©è®°å½•ï¼ˆå·²ç»è¿‡ç¨€ç–é‡‡æ ·ï¼Œæ— éœ€å†æˆªæ–­ï¼‰
            prompt += f"""
## ğŸ’¬ èŠå¤©è®°å½•ï¼ˆç”¨äºåˆ†æè¯´è¯é£æ ¼ï¼‰
{chat_sample}
"""
        
        prompt += """
è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½æœ‰è¶£åˆ›æ„çš„ä¸ªäººå¹´åº¦æ€»ç»“ï¼
"""
        
        return prompt
    
    def _build_group_and_network_prompt(self, group_stats: Dict[str, Any],
                                        network_stats: Dict[str, Any] = None,
                                        chat_sample: str = "") -> str:
        """
        æ„å»ºç¾¤ä½“å’Œç½‘ç»œèåˆæ€»ç»“çš„ç”¨æˆ·æç¤ºè¯
        
        Args:
            group_stats: ç¾¤ä½“ç»Ÿè®¡æ•°æ®
            network_stats: ç½‘ç»œç»Ÿè®¡æ•°æ®ï¼ˆå¯é€‰ï¼‰
            chat_sample: èŠå¤©æ ·æœ¬
        
        Returns:
            èåˆçš„ prompt å­—ç¬¦ä¸²
        """
        # ç¾¤ä½“ç»Ÿè®¡æ•°æ®
        total_messages = group_stats.get('total_messages', 0)
        daily_avg = group_stats.get('daily_average', 0)
        peak_hours = group_stats.get('peak_hours', [])
        core_members = group_stats.get('core_members', [])
        active_members = group_stats.get('active_members', [])
        normal_members = group_stats.get('normal_members', [])
        lurkers = group_stats.get('lurkers', [])
        hot_words = group_stats.get('hot_words', [])
        monthly_trend = group_stats.get('monthly_trend', {})
        text_ratio = group_stats.get('text_ratio', 0)
        image_ratio = group_stats.get('image_ratio', 0)
        emoji_ratio = group_stats.get('emoji_ratio', 0)
        
        # æ–°å¢çš„æ—¶é—´ç»Ÿè®¡æ•°æ®ï¼ˆéå¸¸é‡è¦ï¼ï¼‰
        hourly_top_users = group_stats.get('hourly_top_users', {})
        weekday_top_users = group_stats.get('weekday_top_users', {})
        weekday_totals = group_stats.get('weekday_totals', {})
        
        # æ ¸å¿ƒæˆå‘˜ä¿¡æ¯
        core_info = []
        for m in core_members[:5]:
            if isinstance(m, dict):
                core_info.append(f"{m.get('name', m.get('qq', '?'))} ({m.get('count', 0)}æ¡)")
            else:
                core_info.append(str(m))
        
        # çƒ­è¯
        hot_words_str = ', '.join([w['word'] for w in hot_words[:15]]) if hot_words else 'æ— '
        
        # å³°å€¼æ—¶é—´
        peak_str = ', '.join([f"{h}:00" for h in peak_hours[:3]]) if peak_hours else 'æœªçŸ¥'
        
        # æ—¶é—´æ®µæ ‡ç­¾
        time_period_labels = {
            0: 'å‡Œæ™¨ ğŸŒ™',
            1: 'å‡Œæ™¨ ğŸŒ™',
            2: 'å‡Œæ™¨ ğŸŒ™',
            3: 'æ¸…æ™¨ ğŸŒ…',
            4: 'æ¸…æ™¨ ğŸŒ…',
            5: 'æ¸…æ™¨ ğŸŒ…',
            6: 'æ—©ä¸Š â˜€ï¸',
            7: 'æ—©ä¸Š â˜€ï¸',
            8: 'æ—©ä¸Š â˜€ï¸',
            9: 'ä¸Šåˆ ğŸŒ¤ï¸',
            10: 'ä¸Šåˆ ğŸŒ¤ï¸',
            11: 'ä¸Šåˆ ğŸŒ¤ï¸',
            12: 'ä¸­åˆ ğŸŒ',
            13: 'ä¸‹åˆ â˜€ï¸',
            14: 'ä¸‹åˆ â˜€ï¸',
            15: 'ä¸‹åˆ â˜€ï¸',
            16: 'å‚æ™š ğŸŒ†',
            17: 'å‚æ™š ğŸŒ†',
            18: 'å‚æ™š ğŸŒ†',
            19: 'æ™šä¸Š ğŸŒ™',
            20: 'æ™šä¸Š ğŸŒ™',
            21: 'æ™šä¸Š ğŸŒ™',
            22: 'æ·±å¤œ ğŸŒƒ',
            23: 'æ·±å¤œ ğŸŒƒ'
        }
        
        # æ ¼å¼åŒ–æ¯å°æ—¶æœ€æ´»è·ƒç”¨æˆ·
        hourly_info = []
        for hour in sorted(hourly_top_users.keys()):
            user = hourly_top_users[hour]
            if isinstance(user, dict):
                hour_int = int(hour)
                period = time_period_labels.get(hour_int, 'æœªçŸ¥')
                hourly_info.append(f"{period} {hour_int:02d}:00 â†’ {user.get('name', user.get('qq', '?'))} ({user.get('count', 0)}æ¡)")
        hourly_str = '\n'.join(hourly_info) if hourly_info else 'æš‚æ— æ•°æ®'
        
        # æ˜ŸæœŸåç§°
        weekday_names = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥']
        
        # æ ¼å¼åŒ–æ¯å‘¨æœ€æ´»è·ƒç”¨æˆ·
        weekday_info = []
        for day in sorted(weekday_top_users.keys()):
            user = weekday_top_users[day]
            if isinstance(user, dict):
                day_name = weekday_names[int(day)] if int(day) < 7 else 'æœªçŸ¥'
                weekday_info.append(f"{day_name}: {user.get('name', user.get('qq', '?'))} ({user.get('count', 0)}æ¡)")
        weekday_str = '\n'.join(weekday_info) if weekday_info else 'æš‚æ— æ•°æ®'
        
        # æ ¼å¼åŒ–æ¯å‘¨æ¶ˆæ¯æ€»é‡
        weekday_totals_info = []
        for day in sorted(weekday_totals.keys()):
            data = weekday_totals[day]
            if isinstance(data, dict):
                count = data.get('count', 0)
                day_name = weekday_names[int(day)] if int(day) < 7 else 'æœªçŸ¥'
                weekday_totals_info.append(f"{day_name}: {count}æ¡")
        weekday_totals_str = '\n'.join(weekday_totals_info) if weekday_totals_info else 'æš‚æ— æ•°æ®'
        
        prompt = f"""
è¯·ä¸ºä»¥ä¸‹ç¾¤èŠç”Ÿæˆä¸€ä»½ç»¼åˆçš„ç¤¾äº¤åˆ†æå¹´åº¦æŠ¥å‘Šï¼š

## ğŸ“Š ç¾¤èŠæ´»åŠ›æ•°æ®

- **æ€»æ¶ˆæ¯æ•°**: {total_messages} æ¡
- **æ—¥å‡æ¶ˆæ¯**: {daily_avg:.1f} æ¡
- **æœ€æ´»è·ƒæ—¶æ®µ**: {peak_str}
- **æ¶ˆæ¯ç±»å‹**: æ–‡å­— {text_ratio*100:.1f}% | å›¾ç‰‡ {image_ratio*100:.1f}% | è¡¨æƒ… {emoji_ratio*100:.1f}%

## ğŸ‘¥ æˆå‘˜æ„æˆ
- **æ ¸å¿ƒæˆå‘˜** (TOP 10%): {len(core_members)} äºº
- **æ´»è·ƒæˆå‘˜** (10%-40%): {len(active_members)} äºº
- **æ™®é€šæˆå‘˜** (40%-80%): {len(normal_members)} äºº
- **æ½œæ°´å‘˜** (Bottom 20%): {len(lurkers)} äºº

## ğŸ‘‘ è¯ç—¨æ’è¡Œæ¦œTOP5
{chr(10).join(core_info) if core_info else 'æš‚æ— æ•°æ®'}

## â° æ—¶æ®µæ´»è·ƒåˆ†æï¼ˆæ¯å°æ—¶æœ€æ´»è·ƒäººç‰©ï¼‰
{hourly_str}

## ğŸ“… å‘¨åº¦æ´»è·ƒåˆ†æï¼ˆæ¯å‘¨æœ€æ´»è·ƒäººç‰©ï¼‰
{weekday_str}

## ğŸ“Š å‘¨åº¦æ¶ˆæ¯æ€»é‡åˆ†æ
{weekday_totals_str}

## ğŸ”¥ ç¾¤èŠçƒ­è¯TOP15
{hot_words_str}

## ğŸ“ˆ æœˆåº¦è¶‹åŠ¿
{json.dumps(monthly_trend, ensure_ascii=False, indent=2)}
"""
        
        # å¦‚æœæœ‰ç½‘ç»œç»Ÿè®¡æ•°æ®ï¼Œæ·»åŠ åˆ° prompt
        if network_stats:
            total_nodes = network_stats.get('total_nodes', 0)
            total_edges = network_stats.get('total_edges', 0)
            density = network_stats.get('density', 0)
            avg_clustering = network_stats.get('avg_clustering_coefficient', 0)
            communities = network_stats.get('communities', [])
            most_popular = network_stats.get('most_popular_user', {})
            most_active_pair = network_stats.get('most_active_pair', {})
            key_connectors = network_stats.get('key_connectors', [])
            
            # ç¤¾äº¤ä¸­å¿ƒ
            popular_info = "æ— "
            if most_popular:
                popular_info = f"{most_popular.get('name', most_popular.get('qq', '?'))} (ä¸­å¿ƒåº¦: {most_popular.get('centrality', 0)*100:.1f}%)"
            
            # æœ€ä½³CP
            pair_info = "æ— "
            if most_active_pair:
                pair = most_active_pair.get('pair', [])
                if len(pair) >= 2:
                    pair_info = f"{pair[0]} â†” {pair[1]} (äº’åŠ¨{most_active_pair.get('weight', 0):.0f}æ¬¡)"
            
            # å…³é”®è¿æ¥è€…
            connectors_info = []
            for c in key_connectors[:3]:
                if isinstance(c, dict):
                    connectors_info.append(f"{c.get('name', c.get('qq', '?'))}")
            
            # ç¤¾åŒºä¿¡æ¯
            community_info = f"{len(communities)} ä¸ªå°åœˆå­" if communities else "æš‚æ— æ˜æ˜¾å°åœˆå­"
            
            prompt += f"""
## ğŸ•¸ï¸ ç¤¾äº¤ç½‘ç»œåˆ†æ

- **å‚ä¸äº’åŠ¨çš„æˆå‘˜**: {total_nodes} äºº
- **äº’åŠ¨å…³ç³»æ•°**: {total_edges} æ¡
- **ç½‘ç»œå¯†åº¦**: {density*100:.1f}%
- **å¹³å‡èšç±»ç³»æ•°**: {avg_clustering:.3f}

### ç¤¾äº¤ä¸­å¿ƒï¼ˆäººæ°”ç‹ï¼‰
{popular_info}

### æœ€ä½³CPï¼ˆäº’åŠ¨æœ€å¤šçš„ç»„åˆï¼‰
{pair_info}

### å…³é”®è¿æ¥è€…ï¼ˆç¤¾äº¤æ¡¥æ¢ï¼‰
{', '.join(connectors_info) if connectors_info else 'æš‚æ— æ˜æ˜¾æ¡¥æ¢äººç‰©'}

### å°åœˆå­åˆ†æ
{community_info}
"""
        
        if chat_sample:
            # æ˜¾ç¤ºé‡‡æ ·çš„èŠå¤©è®°å½•ï¼ˆå·²ç»è¿‡ç¨€ç–é‡‡æ ·ï¼Œæ— éœ€å†æˆªæ–­ï¼‰
            prompt += f"""
## ğŸ’¬ èŠå¤©è®°å½•æ ·æœ¬ï¼ˆæ ¸å¿ƒç´ æï¼ç”¨äºåˆ†æç¾¤å‹æ€§æ ¼å’Œè¯´è¯é£æ ¼ï¼‰

âš ï¸ **é‡è¦æç¤º**ï¼šä»¥ä¸‹èŠå¤©è®°å½•æ˜¯åˆ†æçš„æ ¸å¿ƒç´ æï¼è¯·ä»”ç»†é˜…è¯»ï¼Œä»ä¸­æå–æ¯ä¸ªäººçš„ï¼š
- è¯´è¯é£æ ¼ï¼ˆæ­£å¼/éšæ„ã€ç®€æ´/å•°å—¦ã€æç¬‘/ä¸¥è‚ƒç­‰ï¼‰
- æ€§æ ¼ç‰¹ç‚¹ï¼ˆå¤–å‘/å†…å‘ã€æ´»æ³¼/ç¨³é‡ã€åæ§½ç³»/æ­£èƒ½é‡ç­‰ï¼‰
- æœ‰ä»£è¡¨æ€§çš„é‡‘å¥æˆ–å£å¤´ç¦…
- äº’åŠ¨æ¨¡å¼ï¼ˆçˆ±æ¥è¯èŒ¬/çˆ±å‘èµ·è¯é¢˜/çˆ±å›åº”åˆ«äººç­‰ï¼‰

{chat_sample}
"""
        
        prompt += """
è¯·æ ¹æ®ä»¥ä¸Šæ•°æ®ï¼Œç”Ÿæˆä¸€ä»½æœ‰è¶£åˆ›æ„çš„ç»¼åˆå¹´åº¦ç¤¾äº¤åˆ†ææŠ¥å‘Šï¼

**ç‰¹åˆ«è¦æ±‚**ï¼š
1. èŠå¤©è®°å½•æ˜¯æŠ¥å‘Šçš„çµé­‚ï¼ŒåŠ¡å¿…ä»ä¸­æŒ–æ˜æ¯ä¸ªäººçš„ç‹¬ç‰¹æ€§æ ¼
2. å¯¹æ´»è·ƒæˆå‘˜çš„æå†™è¦å…·ä½“ç”ŸåŠ¨ï¼Œè®©ç¾¤å‹çœ‹äº†èƒ½ä¼šå¿ƒä¸€ç¬‘
3. èåˆç¾¤ä½“çƒ­åº¦å’Œäººé™…æ¸©åº¦ï¼Œæ—¢ä½“ç°æ´»åŠ›æŒ‡æ•°ï¼Œä¹Ÿè¦æŒ–æ˜äººæƒ…å‘³
4. æ•°æ®åˆ†æå’ŒèŠå¤©å†…å®¹åˆ†æè¦ç»“åˆèµ·æ¥ï¼Œä¸è¦åªå †æ•°å­—
"""
        
        return prompt
    
